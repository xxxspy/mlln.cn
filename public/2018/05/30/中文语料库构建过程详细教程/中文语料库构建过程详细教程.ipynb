{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 中文语料库构建过程详细教程\n",
    "date: 2018-05-30 18:17:55\n",
    "tags: [python, 语料库, gensim]\n",
    "toc: true\n",
    "\n",
    "---\n",
    "<span></span>\n",
    "<!-- more -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简介\n",
    "\n",
    "今天我想简单记录一下自己构建语料库的过程, 方便自己查看和方便协作. 在工作中我们经常遇到一个问题就是每个研究者都有自己的语料库, 存储格式不同, 有用mysql这种结构化数据库的, 也有mogodb这种文档型数据库, 还有更多的是使用文本文件, 不管哪种形式, 都会导致数据交换出现困难. 他人使用这个语料库的时候需要自己写语料库的预处理函数, 否则语料库是不能进入计算的. 为了减少这种不必要的重复劳动, 我们就使用`gensim.corpora.textcorpus.TextDirectoryCorpus`类来管理语料. 也就是说, 我们的语料保存在文件夹中, 设置`lines_are_documents=False`来保证每个文件是一篇文档. 如果我们都以相同的方式管理语料库, 那么我们的协作就更顺畅.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目录结构\n",
    "\n",
    "因为`TextDirectoryCorpus`可以支持嵌套的文件夹, 只要指定`max_depth`和`min_depth`两个参数就能控制文件夹深度. 我通常是使用一个文件夹放所有文本文件. 比如我有这样一个目录作为语料库:\n",
    "\n",
    "```\n",
    "└─test-corpus         # 根目录\n",
    "    │  dictionary.model  # 字典数据\n",
    "    │  meta.csv       # 元信息\n",
    "    │\n",
    "    └─corpus         # 存放文档\n",
    "            01.txt\n",
    "            02.txt\n",
    "            03.txt\n",
    "            04.txt\n",
    "            05.txt\n",
    "            06.txt\n",
    "            07.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'.*\\.txt', re.UNICODE)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.compile('.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextDirectoryCorpus简单使用\n",
    "\n",
    "先看以下基本用法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库计数: 7\n",
      "第一条: ['树中路径的交集图']\n"
     ]
    }
   ],
   "source": [
    "# 引入用到的库\n",
    "from gensim.corpora.textcorpus import TextDirectoryCorpus\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.utf-8')\n",
    "# 数据所在路径\n",
    "dpath = r'D:\\mysites\\notebooks\\data\\test-corpus'\n",
    "\n",
    "\n",
    "class MyTextDirCorpus(TextDirectoryCorpus):\n",
    "    # 为了强制使用'utf8'编码, 我们复写了这个方法\n",
    "    def getstream(self):\n",
    "        \"\"\"Yield documents from the underlying plain text collection (of one or more files).\n",
    "        Each item yielded from this method will be considered a document by subsequent\n",
    "        preprocessing methods.\n",
    "        If `lines_are_documents` was set to True, items will be lines from files. Otherwise\n",
    "        there will be one item per file, containing the entire contents of the file.\n",
    "        \"\"\"\n",
    "        num_texts = 0\n",
    "        for path in self.iter_filepaths():\n",
    "            with open(path, 'rt', encoding='utf8') as f:\n",
    "                if self.lines_are_documents:\n",
    "                    for line in f:\n",
    "                        yield line.strip()\n",
    "                        num_texts += 1\n",
    "                else:\n",
    "                    content = f.read().strip()\n",
    "                    yield content\n",
    "                    num_texts += 1\n",
    "\n",
    "        self.length = num_texts\n",
    "\n",
    "# 实例化一个语料库, \n",
    "# 遍历的最小深度是1\n",
    "# 设置lines_are_documents为False\n",
    "# 后缀为txt的文件才会被当作是一个文档\n",
    "corpus = MyTextDirCorpus(dpath, min_depth=1, pattern='.*\\.txt',  lines_are_documents=False)\n",
    "print('语料库计数:', len(corpus))\n",
    "print('第一条:',next(corpus.get_texts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词和词典生成\n",
    "\n",
    "为了让同事之间的工作更有一致性, 我们通常要预先对语料库进行分词, 使得不同的人具有相同的分词结果. 我们就用最简单的方法, 使用pyltp模块进行分词, 然后生成一个词典.\n",
    "\n",
    "我们先来看看pyltp是如何进行分词的:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 实验室\t中\t测量\t无序\t响应\t中\t时间\n"
     ]
    }
   ],
   "source": [
    "# 分词测试\n",
    "from pyltp import Segmentor\n",
    "new_doc = '实验室中测量无序响应中时间'\n",
    "model_path = r'D:\\mysites\\text-characters\\tcharacters\\ltp\\ltp_data\\cws.model'\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(model_path)\n",
    "new_tokens = segmentor.segment(new_doc)\n",
    "print('Tokens:', '\\t'.join(new_tokens))\n",
    "segmentor.release()  # 释放模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个分词器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'中': 0,\n",
       " '交集图': 1,\n",
       " '树': 2,\n",
       " '的': 3,\n",
       " '路径': 4,\n",
       " '\\ufeff': 5,\n",
       " '无序': 6,\n",
       " '生成': 7,\n",
       " '随机二进制': 8,\n",
       " '与': 9,\n",
       " '关系': 10,\n",
       " '响应': 11,\n",
       " '感知': 12,\n",
       " '时间': 13,\n",
       " '测量': 14,\n",
       " '用户': 15,\n",
       " '误差': 16,\n",
       " 'eps': 17,\n",
       " '人体': 18,\n",
       " '和': 19,\n",
       " '测试': 20,\n",
       " '系统': 21,\n",
       " '系统工程': 22,\n",
       " '界面': 23,\n",
       " '管理': 24,\n",
       " '对': 25,\n",
       " '看法': 26,\n",
       " '计算机': 27,\n",
       " '调查': 28,\n",
       " '实验室': 29,\n",
       " '应用': 30}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyTextDirCorpus(TextDirectoryCorpus):\n",
    "\n",
    "    def __init__(self, input, **kwargs):\n",
    "        kwargs['tokenizer'] = self.tokenizer\n",
    "        super().__init__(input, **kwargs)\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        if not hasattr(self, '_segmentor'):\n",
    "            model_path = r'D:\\mysites\\text-characters\\tcharacters\\ltp\\ltp_data\\cws.model'\n",
    "            segmentor = Segmentor()  # 初始化实例\n",
    "            segmentor.load(model_path)\n",
    "            self._segmentor = segmentor\n",
    "        segmentor = self._segmentor\n",
    "        return segmentor.segment(text)\n",
    "    \n",
    "    def __del__(self):\n",
    "        '''释放资源'''\n",
    "        if hasattr(self, '_segmentor'):\n",
    "            self._segmentor.release()\n",
    "        try:\n",
    "            super().__del__()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    # 为了强制使用'utf8'编码, 我们复写了这个方法\n",
    "    def getstream(self):\n",
    "        \"\"\"Yield documents from the underlying plain text collection (of one or more files).\n",
    "        Each item yielded from this method will be considered a document by subsequent\n",
    "        preprocessing methods.\n",
    "        If `lines_are_documents` was set to True, items will be lines from files. Otherwise\n",
    "        there will be one item per file, containing the entire contents of the file.\n",
    "        \"\"\"\n",
    "        num_texts = 0\n",
    "        for path in self.iter_filepaths():\n",
    "            with open(path, 'rt', encoding='utf8') as f:\n",
    "                if self.lines_are_documents:\n",
    "                    for line in f:\n",
    "                        yield line.strip()\n",
    "                        num_texts += 1\n",
    "                else:\n",
    "                    content = f.read().strip()\n",
    "                    yield content\n",
    "                    num_texts += 1\n",
    "\n",
    "        self.length = num_texts\n",
    "\n",
    "# 实例化一个语料库\n",
    "# 注意我们传入了一个token_filters参数, 实际上是一个空列表, 意思是, 不要过滤词, 所有词都要\n",
    "# 当然在有需要的情况下, 我们需要定义自己的token_filters\n",
    "corpus = MyTextDirCorpus(dpath, min_depth=1, \n",
    "                         pattern='.*\\.txt',  \n",
    "                         lines_are_documents=False, \n",
    "                         tokenizer=tokenizer, \n",
    "                         token_filters=[])\n",
    "# 保存词典到本地硬盘\n",
    "dict_path = r'D:\\mysites\\notebooks\\data\\test-corpus\\dictionary.model'\n",
    "corpus.dictionary.save_as_text(fname=dict_path)\n",
    "# 查看词典内容\n",
    "corpus.dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以在`dict_path`这个路径下看到我们的字典内容, 使用任意一个文本编辑器即可打开, 如下图, 三列数据分别表示id/词/计数.\n",
    "\n",
    "<img src=\"images/dictionary-example.png\" title=\"语料库的字典\" class=\"img-thumbnail\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词典的使用\n",
    "\n",
    "生成了词典以后, 我们以后再使用语料库的时候, 可以不必每次都重新计算词典, 这个过程非常耗费资源, 所以我们使用已经存储在硬盘的词典数据."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 µs ± 435 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 预加载字典\n",
    "dic = Dictionary.load_from_text(dict_path)\n",
    "MyTextDirCorpus(dpath, \n",
    "                dictionary=dic,\n",
    "                min_depth=1, \n",
    "                 pattern='.*\\.txt',  \n",
    "                 lines_are_documents=False, \n",
    "                 tokenizer=tokenizer, \n",
    "                 token_filters=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 116 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 每次生成词典\n",
    "MyTextDirCorpus(dpath, \n",
    "                min_depth=1, \n",
    "                 pattern='.*\\.txt',  \n",
    "                 lines_are_documents=False, \n",
    "                 tokenizer=tokenizer, \n",
    "                 token_filters=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的测试, 我们发现两种方式的差别很大, 这还是在小语料库上测试的, 如果是大语料库, 这个差别就是几个数量级的差别了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预先分词\n",
    "\n",
    "其实, 很多人协作的情况下, 保证大家的分词结果的一致性是很有必要的, 不然很多结果都没有可比性. 与其每个人都分一遍词, 不如从一开始就分好词. 所以我们可以把词分好, 然后保存到硬盘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "dic = Dictionary.load_from_text(dict_path)\n",
    "corpus = MyTextDirCorpus(dpath, \n",
    "                dictionary=dic,\n",
    "                min_depth=1, \n",
    "                 pattern='.*\\.txt$',  \n",
    "                 lines_are_documents=False, \n",
    "                 tokenizer=tokenizer, \n",
    "                 token_filters=[])\n",
    "\n",
    "for fpath in corpus.iter_filepaths():\n",
    "    fpath = Path(fpath)\n",
    "    token_path = fpath.parent / (fpath.name + '.cached_tokens')\n",
    "    txt = fpath.read_text(encoding='utf8').strip()\n",
    "    tokens = corpus.tokenizer(txt)\n",
    "    token_path.write_bytes(pickle.dumps(list(tokens)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用预先分好的词, 可以再给我们的`MyTextDirCorpus`添加一个方法`get_texts_from_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTextDirCorpus(TextDirectoryCorpus):\n",
    "\n",
    "    def __init__(self, input, **kwargs):\n",
    "        kwargs['tokenizer'] = self.tokenizer\n",
    "        super().__init__(input, **kwargs)\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        if not hasattr(self, '_segmentor'):\n",
    "            model_path = r'D:\\mysites\\text-characters\\tcharacters\\ltp\\ltp_data\\cws.model'\n",
    "            segmentor = Segmentor()  # 初始化实例\n",
    "            segmentor.load(model_path)\n",
    "            self._segmentor = segmentor\n",
    "        segmentor = self._segmentor\n",
    "        return segmentor.segment(text)\n",
    "    \n",
    "    def __del__(self):\n",
    "        '''释放资源'''\n",
    "        if hasattr(self, '_segmentor'):\n",
    "            self._segmentor.release()\n",
    "        try:\n",
    "            super().__del__()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    # 为了强制使用'utf8'编码, 我们复写了这个方法\n",
    "    def getstream(self):\n",
    "        \"\"\"Yield documents from the underlying plain text collection (of one or more files).\n",
    "        Each item yielded from this method will be considered a document by subsequent\n",
    "        preprocessing methods.\n",
    "        If `lines_are_documents` was set to True, items will be lines from files. Otherwise\n",
    "        there will be one item per file, containing the entire contents of the file.\n",
    "        \"\"\"\n",
    "        num_texts = 0\n",
    "        for path in self.iter_filepaths():\n",
    "            with open(path, 'rt', encoding='utf8') as f:\n",
    "                if self.lines_are_documents:\n",
    "                    for line in f:\n",
    "                        yield line.strip()\n",
    "                        num_texts += 1\n",
    "                else:\n",
    "                    content = f.read().strip()\n",
    "                    yield content\n",
    "                    num_texts += 1\n",
    "\n",
    "        self.length = num_texts\n",
    "        \n",
    "    def get_texts_from_tokens(self):\n",
    "        for fpath in self.iter_filepaths():\n",
    "            fpath = Path(fpath)\n",
    "            token_path = fpath.parent / (fpath.name + '.cached_tokens')\n",
    "            yield pickle.loads(token_path.read_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试以下我们新的方法是不是更节省时间:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 ms ± 2.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "dic = Dictionary.load_from_text(dict_path)\n",
    "corpus = MyTextDirCorpus(dpath, \n",
    "                dictionary=dic,\n",
    "                min_depth=1, \n",
    "                 pattern='.*\\.txt$',  \n",
    "                 lines_are_documents=False, \n",
    "                 tokenizer=tokenizer, \n",
    "                 token_filters=[])\n",
    "\n",
    "for i in corpus.get_texts():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969 µs ± 7.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "dic = Dictionary.load_from_text(dict_path)\n",
    "corpus = MyTextDirCorpus(dpath, \n",
    "                dictionary=dic,\n",
    "                min_depth=1, \n",
    "                 pattern='.*\\.txt$',  \n",
    "                 lines_are_documents=False, \n",
    "                 tokenizer=tokenizer, \n",
    "                 token_filters=[])\n",
    "\n",
    "for i in corpus.get_texts_from_tokens():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试显示, 时间还是差了几个数量级, 而且第二种方法的好处是, 我们使用语料库时不必再依赖分词模块`pyltp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "现在我们基本上已经构建了一个标准统一的语料库, 而且预先进行了分词和词典生成. 现在我们把所有用到的代码都封装到`MyTextDirCorpus`类中. 看代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: mlln.cn\n",
    "# email: xxxspy@126.com\n",
    "# qq: 675495787\n",
    "\n",
    "class MyTextDirCorpus(TextDirectoryCorpus):\n",
    "\n",
    "    def __init__(self, input, **kwargs):\n",
    "        kwargs['tokenizer'] = self.tokenizer\n",
    "        super().__init__(input, **kwargs)\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        if not hasattr(self, '_segmentor'):\n",
    "            model_path = r'D:\\mysites\\text-characters\\tcharacters\\ltp\\ltp_data\\cws.model'\n",
    "            segmentor = Segmentor()  # 初始化实例\n",
    "            segmentor.load(model_path)\n",
    "            self._segmentor = segmentor\n",
    "        segmentor = self._segmentor\n",
    "        return segmentor.segment(text)\n",
    "    \n",
    "    def __del__(self):\n",
    "        '''释放资源'''\n",
    "        if hasattr(self, '_segmentor'):\n",
    "            self._segmentor.release()\n",
    "        try:\n",
    "            super().__del__()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    # 为了强制使用'utf8'编码, 我们复写了这个方法\n",
    "    def getstream(self):\n",
    "        \"\"\"Yield documents from the underlying plain text collection (of one or more files).\n",
    "        Each item yielded from this method will be considered a document by subsequent\n",
    "        preprocessing methods.\n",
    "        If `lines_are_documents` was set to True, items will be lines from files. Otherwise\n",
    "        there will be one item per file, containing the entire contents of the file.\n",
    "        \"\"\"\n",
    "        num_texts = 0\n",
    "        for path in self.iter_filepaths():\n",
    "            with open(path, 'rt', encoding='utf8') as f:\n",
    "                if self.lines_are_documents:\n",
    "                    for line in f:\n",
    "                        yield line.strip()\n",
    "                        num_texts += 1\n",
    "                else:\n",
    "                    content = f.read().strip()\n",
    "                    yield content\n",
    "                    num_texts += 1\n",
    "\n",
    "        self.length = num_texts\n",
    "        \n",
    "    def get_texts_from_tokens(self):\n",
    "        for fpath in self.iter_filepaths():\n",
    "            fpath = Path(fpath)\n",
    "            token_path = fpath.parent / (fpath.name + '.cached_tokens')\n",
    "            yield pickle.loads(token_path.read_bytes())\n",
    "            \n",
    "    def save_tokens(self):\n",
    "        '''保存tokens到硬盘, 只需要运行一次'''\n",
    "        for fpath in self.iter_filepaths():\n",
    "            fpath = Path(fpath)\n",
    "            token_path = fpath.parent / (fpath.name + '.cached_tokens')\n",
    "            txt = fpath.read_text(encoding='utf8').strip()\n",
    "            tokens = self.tokenizer(txt)\n",
    "            token_path.write_bytes(pickle.dumps(list(tokens)))\n",
    "    \n",
    "    def save_dictionary(self, dpath):\n",
    "        '''把字典保存到硬盘'''\n",
    "        self.dictionary.save_as_text(fname=dpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 本为由mlln.cn原创, 转载请注明出处(mlln.cn), 本站保留版权. 欢迎童鞋在下方留言反馈."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
