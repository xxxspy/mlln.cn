{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 文本向量化理论和实践教程\n",
    "date: 2018-08-17 8:17:55\n",
    "tags: [python, 文本挖掘]\n",
    "toc: true\n",
    "xiongzhang: true\n",
    "xiongzhang_images: [main.jpg]\n",
    "\n",
    "---\n",
    "<span></span>\n",
    "<!-- more -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系列文章\n",
    "\n",
    "这个是系列博客, 所有文章链接都列在这里, 并持续更新中。\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"http://mlln.cn/2018/08/17/文本向量化理论和实践教程/\" >文本向量化理论和实践教程</a></li>\n",
    "<li><a href=\"http://mlln.cn/2018/08/17/文本向量系列-如何基于拼音构建字向量/\" >文本向量系列-如何基于拼音构建字向量</a></li>\n",
    "<li><a href=\"http://mlln.cn/2018/08/18/文本向量系列-如何基于笔画+拼音构建字向量/\" >文本向量系列-如何基于笔画+拼音构建字向量/</a></li>\n",
    "<li><a href=\"http://mlln.cn/2018/08/18/文本向量系列-如何基于词频矩阵和TF-IDF权重构建词向量/\" >文本向量系列-如何基于词频矩阵和TF-IDF权重构建词向量/</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主要内容\n",
    "\n",
    "文本向量化是指将文本表示成低维、稠密、实数向量的一种方法。随着深度学习技术的广泛应用,基于神经网络的文本向量化成为自然语言处理领域的研究热点,尤其是对单词的向量化研究。文本按照粒度大小将文本向量化划分为多个层次:\n",
    "\n",
    "- 字的向量化\n",
    "- 词的向量化\n",
    "- 句子/篇章向量化\n",
    "\n",
    "这篇文章主要介绍这些向量化方法的理论知识, 而接下来的几篇文章会使用python程序来实践这些方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字的向量化\n",
    "\n",
    "#### 基于拼音\n",
    "\n",
    "用拼音来表示字向量的方法我见的比较少, 也没有见过类似的研究, 我这里只是介绍一下这个方法, 但是并不能评价这个方法的好坏. 在一些英文语料的研究中见过一些研究者使用字母来形成词向量的, 因为总共有26个英文字母, 所以一个词/拼音可以表示为26维的字母频率向量, 这个我们后面用案例来详解。\n",
    "\n",
    "#### 基于笔画\n",
    "\n",
    "中文的词都是由字构成的, 所以有一些研究人员更关心如何用向量来表示一个字, 比如<基于汉字固有属性的中文字向量方法研究>, 该方法结合中文汉字的构词和拼音属性,将中文汉字映射为一个仅32维的空间向量,最后使用卷积神经网络进行语义提取并进行相似性计算。实验结果表明,与现有的短文本相似性计算方法相比,该方法在算法性能及准确率上均有较大的提高。\n",
    "\n",
    "\n",
    "#### 基于其他词向量方法\n",
    "字和词具有相似性, 所以很多适用于词向量化方法也适用于字向量, 但是为了保持和英文文献的一致性, 我们把这些方法放在词向量化部分来介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量化\n",
    "\n",
    "词是语义的基本单位, 所以绝大部分向量化的方法都是针对词的。\n",
    "\n",
    "#### one-hot稀疏表征\n",
    "\n",
    "该方法结合中文汉字的构词和拼音属性,将中文汉字映射为一个仅32维的空间向量,最后使用卷积神经网络进行语义提取并进行相似性计算。实验结果表明,与现有的短文本相似性计算方法相比,该方法在算法性能及准确率上均有较大的提高。这种方法很简单, 假如你的词库由3个词构成: 我 /爱/你, 那么你可以用维度为3的向量分别表示这三个词:\n",
    "\n",
    "```\n",
    "我 = [1 0 0]\n",
    "爱 = [0 1 0]\n",
    "你 = [0 0 1]\n",
    "```\n",
    "\n",
    "注意: One-hot的表征方式无法反映语义信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量的分布(distributional)表示\n",
    "\n",
    "如何刻画词与上下文之间的关系是区别各种分布表示方法的关键，下面将简单介绍词向量的分布表示方法。\n",
    "\n",
    "\n",
    "##### 篇章/词频矩阵\n",
    "\n",
    "基于矩阵的分布表示通常又称为语义分布模型ＰＵ，该方法的主要思想是构建一个共现矩阵，矩阵的每行对应一个单词，每列表示一种上下文(通常是一篇文章)，而每个元素的值为对应单词与上下文在语料库中的共现次数。因此，每个单词可由矩阵中对应的行向量表示，而任意两个单词的相似性可直接由它们向量的相似性衡量。\n",
    "\n",
    "下面就是一个共现矩阵, 每一行就是一个词向量:\n",
    "\n",
    "<table>\n",
    "    <tr><td></td><td>文档1</td><td>文档2</td><td>文档3</td><td>...</td></tr>\n",
    "    <tr><td>词1</td><td>1</td><td>0</td><td>3</td><td>...</td></tr>\n",
    "    <tr><td>词2</td><td></td><td>1</td><td>0</td><td>...</td></tr>\n",
    "    <tr><td>词3</td><td>1</td><td>0</td><td>0</td><td>...</td></tr>\n",
    "    <tr><td>.</td><td>13</td><td>4</td><td>3</td><td>...</td></tr>\n",
    "</table>\n",
    "\n",
    "##### TF-IDF权重\n",
    "\n",
    "TF-IDF基于上面提到的共现矩阵。从实际操作角度来说, TF-IDF只是对共现矩阵进行了加权。\n",
    "\n",
    "传统经典模型TF-IDF以及一些基于它改进的方法:主要思想是通过提取文本中词语的权重来标识句子, 使文本构成向量表达。权重主要由两部分组成, 即该词语在文本中的频率 (term frequency, TF) 与反文档频率 (inverse document frequency, IDF) 。它衡量了一个词的常见程度，TF-IDF的假设是：如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出，那么它很可能就反映了这篇文章的特性，因此要提高它的权值。然而这种方法太过于依赖词语的共现, 加上本身短文本消息就由很少的字组成, 往往实际应用中得不到很好的效果。因为两个文本消息可能没有共同的词语但也可以语义相关, 相反如果两个文本消息有一些共同的词语也不一定语义相关。如”富士苹果很好吃, 赶紧买”, “苹果六代真好用, 赶紧买”和”乔布斯逝世了”。\n",
    "\n",
    "\n",
    "\n",
    "##### LSA潜在语义分析\n",
    "\n",
    "基于共现矩阵的词向量表示一般具有很高的维度，且非常稀疏，导致其很难直接应用于各种自然语言处理任务中。使用降维技术可将其转换为相对低维、稠密的向量，并减少噪音影响。常用的降维技术有奇异值分解（SVD），非负矩阵分解（NMF），典型关联分析（CAA）和主成分分析（PCA）等。基于上述降维技术，学术界提出了很多生成词向量表示的具体方法，比如经典的LSA算法，其基本思想是使用SVD对＂词－文档＂矩阵分解，进而得到单词的低维向量表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 基于神经网络的词表示\n",
    "\n",
    "神经网络的词向量表示法是基于一个哲学: 词的上下文环境决定了词的意义, 所以在训练词向量的时候, 是通过词的上下文来计算词向量, 因而这样的词向量也具有了上下文的语义信息。\n",
    "\n",
    "基于神经网络生成的词表示一般称为词向量、词嵌入（wordembeding），狭义上，与分布表示（distributional representation）相对，也称为分布式表示（distributed representation）。该表示将每一单词映射到一个低维、稠密的实数向量上，向量的每一维代表单词的潜在特征，且该特征能够捕捉到有用的语法和语义属性。词向量的最大优势在于我们可Ｗ相对快速、有效地根据向量间的距离（比如余弦距离）判断单词的相似性，原因在于语义或语法相似的单词拥有相似的向量表示。词向量的构建主要依赖于神经网络模垫，神经网络能够非常灵活地以线性组合方式表示任意n-gram词组，且参数以线性速度増长，相对于基于矩阵和聚类的分布表示，词向量能够捕捉到更多复杂、有用的语义信息。\n",
    "\n",
    "基于神经网络的词嵌入也有很多不同的方法, 我们会在实操环节具体的解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 短文本向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
