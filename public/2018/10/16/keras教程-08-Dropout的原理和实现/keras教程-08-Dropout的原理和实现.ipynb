{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: keras教程-08-Dropout的原理和实现\n",
    "date: 2018-10-16 20:17:55\n",
    "tags: [keras教程]\n",
    "toc: true\n",
    "xiongzhang: false\n",
    "\n",
    "---\n",
    "<span></span>\n",
    "<!-- more -->\n",
    "\n",
    "\n",
    "本文代码运行环境:\n",
    "\n",
    "- windows10\n",
    "- python3.6\n",
    "- jupyter notebook\n",
    "- tensorflow 1.x\n",
    "- keras 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络和深度学习模型的简单而强大的正则化技术是`Dropout`。在这篇文章中，您将发现dropout正则化技术以及如何将其应用于Keras的模型中。\n",
    "\n",
    "阅读这篇文章后你会知道：\n",
    "\n",
    "- dropout正则化技术原理\n",
    "- 如何在输入层上使用dropout。\n",
    "- 如何在隐藏图层上使用dropout。\n",
    "- 如何根据不同的问题情景调整dropout水平"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout正则化原理\n",
    "\n",
    "Dropout 是Srivastava等人提出的神经网络模型的正则化技术。在2014年的论文 Dropout: A Simple Way to Prevent Neural Networks from Overfitting ([下载 PDF](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) )\n",
    "\n",
    "Dropout是一种在训练过程中随机忽略神经元的技术。他们随机“Dropout”, 意味着它们对下游神经元激活的贡献在正向通过时暂时消除，并且任何权重更新都不会发生于这些神经元。\n",
    "\n",
    "当神经网络训练时，不同神经元会根据不同特征来调整自己的参数。神经元变得依赖于这种特殊的特征, 有些特省甚至是有害的，如果学习太多这些特征可能导致模型泛化能力下降, 无法适应训练数据以外的数据。\n",
    "\n",
    "您可以想象，如果在训练期间神经元随机掉出网络，其他神经元将不得不承担这些缺失神经元的工作, 分担缺失神经元的一部分特征表示。我们相信, 这会导致网络学习多个独立的内部表示。\n",
    "\n",
    "其结果是网络对神经元的特定权重变得不那么敏感。这反过来导致网络能够更好地概括并且不太可能过度拟合训练数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras中的Dropout层\n",
    "\n",
    "每次训练时, 以一定的概率(比如20%)丢弃一些神经元很容易实现。这就是在Keras中实施Dropout的方式。 Dropout仅在模型训练期间使用，在评估模型的效果时不使用。\n",
    "\n",
    "接下来，我们将探讨在Keras中使用Dropout的几种不同方法。\n",
    "\n",
    "这些示例将使用Sonar数据集。这是一个二元分类问题，其目标是从声纳信息中正确识别岩石和模拟地雷。它是神经网络的一个很好的测试数据集，因为所有输入值都是数字的并且具有相同的单位。\n",
    "\n",
    "可以从UCI机器学习库下载数据集。您可以将声纳数据集放在notebook所在目录下的data文件夹中，文件名为`sonar.csv`。\n",
    "\n",
    "我们将使用scikit-learn中的10折交叉验证法来评估的模型，以便更好地梳理结果中的差异。\n",
    "\n",
    "输入是60维的向量，输出是1维的。输入值在用于训练之前已标准化。基线神经网络模型具有两个隐藏层，第一层具有60个单元，第二层具有30个。使用随机梯度下降法进行模型训练, 并且设置相对较低的学习率。\n",
    "\n",
    "下面列出了完整的基线模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 81.66% (6.89%)\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model on the Sonar Dataset\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在输入层使用Dropout\n",
    "\n",
    "在下面的示例中，我们在输入层和第一个隐藏层之间添加一个新的Dropout层。dropout值设置为20％，这意味着每个更新周期中将随机排除五分之一输入。\n",
    "\n",
    "此外，正如Dropout原始论文中所建议的那样，对每个隐藏层的权重施加约束，确保权重的最大范数不超过值3.这可以通过在Dropout层上设置kernel_constraint参数来完成。\n",
    "\n",
    "学习率提高了一个数量级，momentum增加到0.9。原始Dropout论文中也推荐了这些学习率。\n",
    "\n",
    "从上面的基线示例继续，下面的代码使用相同的网络, 只是在输入层添加了dropout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 85.09% (5.87%)\n"
     ]
    }
   ],
   "source": [
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dropout(0.2, input_shape=(60,)))\n",
    "\tmodel.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在隐藏层使用Dropout\n",
    "\n",
    "\n",
    "Dropout可以应用于网络中的隐藏层神经元。\n",
    "\n",
    "在下面的示例中，Dropout应用于两个隐藏层之间以及最后一个隐藏层和输出层之间。再次使用20％的dropout值，以及对这些层的权重约束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden: 81.68% (4.85%)\n"
     ]
    }
   ],
   "source": [
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，针对此问题以及所选网络配置，在隐藏层中使用Dropout不会提升性能。可能需要额外的训练或者需要进一步调整学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Dropout的一些技巧\n",
    "\n",
    "关于Dropout的原始论文提供了一套标准机器学习问题的实验结果。因此，在实践中使用Dropout时，他们提供了许多有用的启发式。\n",
    "\n",
    "- 通常，使用20％-50％神经元的小dropout值，20％提供良好的起点。概率太低具有最小的影响，而太高的值导致网络的学习不足。\n",
    "- 使用更大的网络。当在较大的网络上使用dropout时，您可能会获得更好的性能，从而为模型提供更多学习独立表示的机会。\n",
    "- 在输入层和隐藏层上使用dropout。在网络的每一层应用Dropout已经显示出良好的结果。\n",
    "- 使用具有衰减和大momentum的学习率。将学习率提高10到100倍，并使用0.9或0.99的momentum值。\n",
    "- 限制网络权重的大小。较大的学习速率可能导致非常大的网络权重。对网络权重的大小施加约束，例如大小为4或5的最大范数正则化已被证明可以改善结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
