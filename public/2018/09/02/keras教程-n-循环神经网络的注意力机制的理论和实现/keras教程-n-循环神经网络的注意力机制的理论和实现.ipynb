{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: keras教程-n-循环神经网络的注意力机制的理论和实现\n",
    "date: 2018-09-02 20:17:55\n",
    "tags: [keras教程]\n",
    "toc: true\n",
    "xiongzhang: false\n",
    "\n",
    "---\n",
    "<span></span>\n",
    "<!-- more -->\n",
    "\n",
    "\n",
    "本文代码运行环境:\n",
    "\n",
    "- windows10\n",
    "- python3.6\n",
    "- jupyter notebook\n",
    "- tensorflow 1.x\n",
    "- keras 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例介绍\n",
    "\n",
    "让我们考虑这样一种情景: 一个序列v(假如维度是10), 里面都是数字, 我们想要预测`abs(v[1:4]-v[5:8])=?`, 也就是序列中, 只有6位数是有效的, 其他的数字都是随机数, 我们想要求这6位数分成的两个三位数的差的绝对值是多少。那么我们可以使用注意力机制, 它可以有效的忽略随机数的干扰, 从而提取得到有效的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理论知识\n",
    "\n",
    "#### 背景知识\n",
    "\n",
    "在一次采访中，现在是OpenAI研究主任的Ilya Sutskever提到，注意机制是最激动人心的进步之一，他们将在这个方向上继续研究。听起来让人兴奋不已。但是什么是注意机制？\n",
    "\n",
    "神经网络中的注意机制基于人类中发现的视觉注意机制。人类视觉注意力得到充分研究，虽然存在不同的模型，但它们都基本上都认为, 注意力能够以“高分辨率”聚焦于图像的某个区域，同时以“低分辨率”感知周围图像，然后随着时间的推移调整焦点。\n",
    "\n",
    "神经网络中的注意力机制历史悠久，特别是在图像识别方面。但是直到2016年才将注意力机制引入NLP（并且越来越多地用于视觉）的循环神经网络架构中。这就是我们将在这篇文章中关注的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意力机制\n",
    "\n",
    "要理解注意力机制需要具备循环神经网络的基础知识, 可以参考我这篇文章: [keras教程-n-10分钟入门LSTM](#), 下面我们用一个机器翻译的例子来做说明。\n",
    "\n",
    "<img src=\"images/rnn-translate.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的流程图就是一个循环神经网络的seq2seq模型, 实现的功能就是把\"Echt dicke Kiste\"翻译成\"Awesome sauce\", 如果你仔细观察就会发现, 这个模型的解码器(蓝色部分)只利用了编码器最后一次循环的状态。也就是说, 想要翻译句子, 必须把所有的信息都保存到一个状态向量里, 这对循环神经网络是有难度的, 尤其当句子很长的时候, 难度更大。\n",
    "\n",
    "解决上面的问题的方法就是使用注意机制。通过注意机制，我们不再尝试将完整的句子编码为固定长度的向量。相反，我们允许解码器在生成的每个步骤“参考”源句子的不同部分。重要的是，我们让模型根据输入句子以及它到目前为止产生的内容来学习要注意的内容。因此，在非常类似的语言（如英语和德语）中，解码器可能会选择按顺序处理事物。在翻译第一个英文单词时注意第一个单词，依此类推。如下所示：\n",
    "\n",
    "<img src=\"images/rnn-attention.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中编码器使用了双向循环神经网络, 但这不是重点, 你可以忽略。 $y_t$代表输出的字符, $x_T$代表输入的字符, $a_t,T$代表权重。重要的是，每个解码器输出字$y_t$现在取决于所有输入状态的加权组合，而不仅仅是最后状态。a是权重，用于定义每个编码器状态的重要性。因此，如果$a_ {3,2}$是一个大数字，这将意味着解码器在产生目标句子的第三个单词时非常关注源句中的第二个状态。 a通常被归一化为总和为1（因此它们是编码器状态的分布）。\n",
    "\n",
    "注意机制的一大优势在于它使我们能够解释和可视化模型正在做什么。例如，通过可视化注意权重矩阵a翻译句子时，我们可以理解模型的翻译方式：\n",
    "\n",
    "<img src=\"images/attention-weights.png\" />\n",
    "\n",
    "在这里我们看到，当从法语翻译成英语时，网络顺序地注意每个输入状态，但有时它在产生输出时同时出现两个单词，例如在翻译“la Syrie”到“叙利亚”时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，这是违反直觉的。人类的注意力是应该是为了节省计算资源。通过专注于一件事，我们可以忽略许多其他事情。但这并不是我们在上述模型中所做的事情。在决定关注什么之前，我们基本上详细查看了所有内容。直观地说，这相当于输出一个翻译的单词，然后返回文本的所有内部存储器，以便决定接下来要生成哪个单词。这似乎是一种浪费，而不是人类正在做的事情。事实上，它更类似于记忆访问，而不是注意，在我看来，这在某种程度上是用词不当（更多内容见下文）。尽管如此，这并没有阻止注意机制变得非常流行并且在许多任务上表现良好。\n",
    "\n",
    "另一种注意方法是使用强化学习来预测要关注的大致位置。这听起来更像人类的注意力，这就是[Recurrent Models of Visual Attention](http://arxiv.org/abs/1406.6247)中所做的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意机制解决的基本问题是它允许网络返回参考输入序列，而不是强迫它将所有信息编码成一个固定长度的向量。正如我上面提到的，我认为注意力有点用词不当。换句话说，注意机制只是让网络访问其内部存储器，这是编码器的隐藏状态。在这种解释中，网络选择从内存中检索的内容，而不是选择“注意”的内容。与典型的存储器不同，这里的存储器访问机制是软的，这意味着网络返回的是存储器位置的加权组合，而不是来自单个离散位置的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras实现注意力机制\n",
    "\n",
    "#### 伪造数据\n",
    "\n",
    "下面我们来创建一些数据, 用于测试我们的模型。创建数据的依据就是上面介绍的案例:\n",
    "\n",
    "> 一个序列v(假如维度是10), 里面都是数字, 我们想要预测abs(v[1:4]-v[5:8])=?, 也就是序列中, 只有6位数是有效的, 其他的数字都是随机数, 我们想要求这6位数分成的两个三位数的差的绝对值是多少。那么我们可以使用注意力机制, 它可以有效的忽略随机数的干扰, 从而提取得到有效的信息。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, Dense, merge, Reshape, Flatten, RepeatVector, Permute, Lambda, LSTM\n",
    "BATCH_SIZE = 12800\n",
    "TIME_STEP = 10\n",
    "DEMESION = 10\n",
    "OUTPUT_LEN = 3\n",
    "num_sample = 300000\n",
    "ONE_HOT = False\n",
    "# 为了结果可重复性\n",
    "np.random.seed(1337)\n",
    "def create_data(n):\n",
    "    '''n: 样本量'''\n",
    "    \n",
    "    X = np.zeros((n, TIME_STEP), dtype='int32')\n",
    "    Y = np.zeros((n,), dtype='int32')\n",
    "    Y_one_hot = np.zeros((n, OUTPUT_LEN, DEMESION), dtype='int32')\n",
    "    X_one_hot = np.zeros((n, TIME_STEP, DEMESION), dtype='int32')\n",
    "\n",
    "    for i in range(n):\n",
    "        row = np.random.randint(0, 10, (TIME_STEP))\n",
    "        x1 = row[1]*100+row[2]*10+row[3]\n",
    "        x2 = row[5]*100+row[6]*10+row[7]\n",
    "        y = abs(x1-x2)\n",
    "        y_str = str(y).zfill(0)\n",
    "        y_digits = [int(s) for s in y_str]\n",
    "        Y[i]=y\n",
    "        X[i] = row\n",
    "        for c, r in enumerate(y_digits):\n",
    "            Y_one_hot[i, c, r] =1\n",
    "        for c,r in enumerate(row):\n",
    "            X_one_hot[i, c, r] =1\n",
    "            \n",
    "#     for i in range(10):\n",
    "#         print(X[i])\n",
    "#         print(X_one_hot[i])\n",
    "    return {\n",
    "        'X': X / 10, \n",
    "        'Y':Y / 1000, \n",
    "        'X_one_hot':X_one_hot,\n",
    "        'Y_one_hot':Y_one_hot,\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 带注意力机制的多层感知机\n",
    "\n",
    "下面我们将要实现一个多层感知机, 并且使用注意机制。下面是带有注意机制的网络架构思路:\n",
    "\n",
    "- 首先输入经过Dense层得到注意力概率分布\n",
    "- 然后注意力概率分布与输入相乘得到加权后的输入\n",
    "- 然后经过Dense层得到输出\n",
    "- 这是一个回归问题, loss使用MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_mul/mul:0\", shape=(?, 10), dtype=float32)\n",
      "output: Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "Input: Tensor(\"input_1:0\", shape=(?, 10), dtype=float32)\n",
      "attention_probs: Tensor(\"attention_vec/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Output: Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n",
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_dense_model():\n",
    "    inputs = Input(shape=(TIME_STEP, ))\n",
    "\n",
    "    # ATTENTION PART STARTS HERE\n",
    "    # 只对时间步骤加权\n",
    "    attention_probs = Dense(TIME_STEP, activation='softmax', name='attention_vec')(inputs)\n",
    "    attention_mul = merge([inputs, attention_probs], output_shape=(TIME_STEP, ), name='attention_mul', mode='mul')\n",
    "    # ATTENTION PART FINISHES HERE\n",
    "    print(attention_mul)\n",
    "    attention_mul = Dense(TIME_STEP, activation='sigmoid')(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    print('output:', output)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    print('Input:', inputs)\n",
    "    print('attention_probs:', attention_probs)\n",
    "    print('Output:', output)\n",
    "    return model,inputs, attention_probs\n",
    "\n",
    "dense_model,inputs, attention_probs = build_dense_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.compile(loss='mean_squared_error',  optimizer='adam',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300000/300000 [==============================] - 7s 22us/step - loss: 0.0269\n",
      "Epoch 2/10\n",
      "300000/300000 [==============================] - 6s 21us/step - loss: 0.0066\n",
      "Epoch 3/10\n",
      "300000/300000 [==============================] - 6s 21us/step - loss: 0.0028\n",
      "Epoch 4/10\n",
      "300000/300000 [==============================] - 6s 21us/step - loss: 0.0023\n",
      "Epoch 5/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0020\n",
      "Epoch 6/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0019\n",
      "Epoch 7/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0018\n",
      "Epoch 8/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0018\n",
      "Epoch 9/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0017\n",
      "Epoch 10/10\n",
      "300000/300000 [==============================] - 6s 20us/step - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b98e6108d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=create_data(num_sample)\n",
    "dense_model.fit([data['X']], data['Y'], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想看注意力的分布, 于是我画了一个柱状图(见下图)。可以看到在第二位和第六位的数的权重最高, 因为这俩数属于最高位数, 决定了Y的大部分成分, 其他部分(第三四六七位)虽然也重要, 但是已经远远不如第第二第六位重要(差一个数量级)。现在loss还在下降, 还有优化的可能性。不过我这里就不继续了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Model(inputs, attention_probs)\n",
    "attention = a_model.predict(data['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.8626799e-02 4.4603109e-01 1.3522616e-06 3.4738071e-02 7.8722491e-04\n",
      " 3.2603509e-08 5.4183584e-03 1.4031818e-02 4.2847097e-01 1.8743974e-03]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(10), np.mean(attention, axis=0))\n",
    "print(np.mean(attention, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实值 [0.483 0.587 0.035 0.009 0.471 0.312 0.319 0.652 0.166 0.746]\n",
      "预测值: [[0.50961006]\n",
      " [0.61784214]\n",
      " [0.08250739]\n",
      " [0.07742871]\n",
      " [0.4838765 ]\n",
      " [0.28396246]\n",
      " [0.28179035]\n",
      " [0.66745037]\n",
      " [0.1289852 ]\n",
      " [0.7194878 ]]\n"
     ]
    }
   ],
   "source": [
    "print('真实值',data['Y'][:10])\n",
    "print('预测值:', dense_model.predict(data['X'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意机制的循环神经网络\n",
    "\n",
    "网络的基本架构如下图, 这是一个多维注意机制架构图, 也就是每个时间步有不同的注意力分配向量。\n",
    "\n",
    "- 输入, 张量的形状是: (BS, TS, I), 分别表示: Batch size, Time Steps, Demension\n",
    "- 转置张量, 使得张量的形状变成:(BS, I, TS)\n",
    "- 全连接层得到注意力权重, 得到的张量形状可以是 (BS, I, TS), 也可以是(BS, TS), 也可以是(BS, I), 这要看你想要让注意力如何分配\n",
    "- 转置张量, 使得张量的维度恢复成(BS, TS, I), 或不需要转置的(BS, TS), (BS, I)\n",
    "- 合并输入和注意力权重, 也就是element-wise的矩阵乘法, 或者row-wise的矩阵乘法\n",
    "\n",
    "<img src=\"images/dense-attention.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过还有一种方式, 是所有时间步共享一个注意力分配向量, 如下图:\n",
    "\n",
    "<img src=\"images/graph_single_attention.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是时候实现一个注意机制的LSTM模型了, 在循环神经网络中, 输入的数据是二维的(TIME_STEP, DEMESION)。也就是说, 我们把每个数字编码成了one-hot向量, 具体做法参考创建数据的代码。而输出仍然是一个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(only_time_step=True):\n",
    "    inputs = Input(shape=(TIME_STEP, DEMESION))\n",
    "    # 转置\n",
    "    a = Permute((2,1))(inputs)\n",
    "    # 现在a的形状是: (DEMENSION, TIME_STEP)\n",
    "    # 目的就是为了得到每个时间步的权重\n",
    "    a = Dense(TIME_STEP, activation='softmax')(a)\n",
    "    if only_time_step:\n",
    "        # 降低一个维度, 使得得到长度为TIME_STEP的向量\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        weight = a\n",
    "        # 出重复这个向量得到一个矩阵便于下一步计算\n",
    "        a = RepeatVector(DEMESION)(a)\n",
    "        # 现在a的形状是(DEMESION, TIME_STEP)\n",
    "    # 转置矩阵, 便于下一步计算乘法\n",
    "    a_probs = Permute((2,1), name='attention_vec')(a)\n",
    "    # 计算加权后的输入\n",
    "    weighted_inputs = merge([inputs, a_probs], name='weighted_inputs', mode='mul')\n",
    "    lstm_units = 64\n",
    "    # 只需要返回最后的结果\n",
    "    output = LSTM(lstm_units, return_sequences=False)(weighted_inputs)\n",
    "    # 经过全连接层\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "    model = Model([inputs], output)\n",
    "    weight_model = Model([inputs], weight)\n",
    "    return model, weight_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "d:\\mysites\\deeplearning.ai-master\\.env\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "lstm_model, weight = build_lstm_model()\n",
    "lstm_model.compile(optimizer='adam', loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300000/300000 [==============================] - 47s 155us/step - loss: 0.0146\n",
      "Epoch 2/10\n",
      "300000/300000 [==============================] - 46s 152us/step - loss: 1.3361e-04\n",
      "Epoch 3/10\n",
      "300000/300000 [==============================] - 46s 153us/step - loss: 1.8170e-05\n",
      "Epoch 4/10\n",
      "300000/300000 [==============================] - 47s 155us/step - loss: 1.0955e-05\n",
      "Epoch 5/10\n",
      "300000/300000 [==============================] - 47s 156us/step - loss: 7.9359e-06\n",
      "Epoch 6/10\n",
      "300000/300000 [==============================] - 48s 159us/step - loss: 6.3272e-06\n",
      "Epoch 7/10\n",
      "300000/300000 [==============================] - 46s 155us/step - loss: 5.5028e-06\n",
      "Epoch 8/10\n",
      "300000/300000 [==============================] - 45s 150us/step - loss: 4.9407e-06\n",
      "Epoch 9/10\n",
      "300000/300000 [==============================] - 45s 150us/step - loss: 4.3552e-06\n",
      "Epoch 10/10\n",
      "300000/300000 [==============================] - 46s 155us/step - loss: 3.9599e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b98f218080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(data['X_one_hot'], data['Y'], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weightV = weight.predict(data['X_one_hot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们可视化一下权重, 可以看到不同时间步的权重, 这个效果并不吃惊, 循环神经网络的能力显然比单纯的感知机模型要好很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(10), np.mean(weightV, axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
